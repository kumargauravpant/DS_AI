{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Question 1.**Write a python program to find out the words after '@' from the below sentences with the use of regex.\n",
    "a = \"xyz@gmail.com\"\n",
    "b = \"abc@yahoo.com\"\n",
    "c = \"xyz@hotmail.com\"\n",
    "d = \"abc@ineuron.ai\"\n",
    "e = \"xyz@outlook.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gmail.com\n",
      "yahoo.com\n",
      "hotmail.com\n",
      "ineuron.ai\n",
      "outlook.com\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(re.findall(r'[^@]+',a)[1])\n",
    "print(re.findall(r'[^@]+',b)[1])\n",
    "print(re.findall(r'[^@]+',c)[1])\n",
    "print(re.findall(r'[^@]+',d)[1])\n",
    "print(re.findall(r'[^@]+',e)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Question 2.** Write a python program with the use of regex to take out the word \"New\" from the following sentence.\n",
    "\n",
    "#[\"New Delhi is the capital of India\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Delhi is the capital of India'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str = \"New Delhi is the capital of India\"\n",
    "re.sub('New','',str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Question 3.** Create one python program in which you have to lowercase the sentence first and than delete digits from the following sentence.\n",
    "\n",
    "#\"In India, 184 people got affected with Corona virus and 4 are died.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in india,  people got affected with corona virus and  are died.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str = 'In India, 184 people got affected with Corona virus and 4 are died.'\n",
    "lower_str = str.lower()\n",
    "re.sub('\\d', '',lower_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Question 4.** Do stemming, lemmatization and tokenization from the following sentence.\n",
    "\n",
    "# \"I hope that, when I have built up my savings, I will be able to travel to Hawai.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "data = 'I hope that, when I have built up my savings, I will be able to travel to Hawai.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'hope', 'that', ',', 'when', 'I', 'have', 'built', 'up', 'my', 'savings', ',', 'I', 'will', 'be', 'able', 'to', 'travel', 'to', 'Hawai', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenize\n",
    "token = nltk.word_tokenize(data)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> porter --> I\n",
      "I --> lancaster --> i\n",
      "I --> snowball --> i\n",
      "-------------------------------------\n",
      "hope --> porter --> hope\n",
      "hope --> lancaster --> hop\n",
      "hope --> snowball --> hope\n",
      "-------------------------------------\n",
      "that --> porter --> that\n",
      "that --> lancaster --> that\n",
      "that --> snowball --> that\n",
      "-------------------------------------\n",
      ", --> porter --> ,\n",
      ", --> lancaster --> ,\n",
      ", --> snowball --> ,\n",
      "-------------------------------------\n",
      "when --> porter --> when\n",
      "when --> lancaster --> when\n",
      "when --> snowball --> when\n",
      "-------------------------------------\n",
      "I --> porter --> I\n",
      "I --> lancaster --> i\n",
      "I --> snowball --> i\n",
      "-------------------------------------\n",
      "have --> porter --> have\n",
      "have --> lancaster --> hav\n",
      "have --> snowball --> have\n",
      "-------------------------------------\n",
      "built --> porter --> built\n",
      "built --> lancaster --> built\n",
      "built --> snowball --> built\n",
      "-------------------------------------\n",
      "up --> porter --> up\n",
      "up --> lancaster --> up\n",
      "up --> snowball --> up\n",
      "-------------------------------------\n",
      "my --> porter --> my\n",
      "my --> lancaster --> my\n",
      "my --> snowball --> my\n",
      "-------------------------------------\n",
      "savings --> porter --> save\n",
      "savings --> lancaster --> sav\n",
      "savings --> snowball --> save\n",
      "-------------------------------------\n",
      ", --> porter --> ,\n",
      ", --> lancaster --> ,\n",
      ", --> snowball --> ,\n",
      "-------------------------------------\n",
      "I --> porter --> I\n",
      "I --> lancaster --> i\n",
      "I --> snowball --> i\n",
      "-------------------------------------\n",
      "will --> porter --> will\n",
      "will --> lancaster --> wil\n",
      "will --> snowball --> will\n",
      "-------------------------------------\n",
      "be --> porter --> be\n",
      "be --> lancaster --> be\n",
      "be --> snowball --> be\n",
      "-------------------------------------\n",
      "able --> porter --> abl\n",
      "able --> lancaster --> abl\n",
      "able --> snowball --> abl\n",
      "-------------------------------------\n",
      "to --> porter --> to\n",
      "to --> lancaster --> to\n",
      "to --> snowball --> to\n",
      "-------------------------------------\n",
      "travel --> porter --> travel\n",
      "travel --> lancaster --> travel\n",
      "travel --> snowball --> travel\n",
      "-------------------------------------\n",
      "to --> porter --> to\n",
      "to --> lancaster --> to\n",
      "to --> snowball --> to\n",
      "-------------------------------------\n",
      "Hawai --> porter --> hawai\n",
      "Hawai --> lancaster --> hawa\n",
      "Hawai --> snowball --> hawai\n",
      "-------------------------------------\n",
      ". --> porter --> .\n",
      ". --> lancaster --> .\n",
      ". --> snowball --> .\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "for word in token:\n",
    "    print(f'{word} --> porter --> {porter.stem(word)}')\n",
    "    print(f'{word} --> lancaster --> {lancaster.stem(word)}')\n",
    "    print(f'{word} --> snowball --> {snowball.stem(word)}')\n",
    "    print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Question 5.** Create one python program from the following sentence.\n",
    "\n",
    "# \"I love NLP, not you\"\n",
    "\n",
    "# output : ['I', 'l', 'N', 'n', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'l', 'N', 'n', 'y']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "data = 'I love NLP, not you'\n",
    "token = nltk.word_tokenize(data)\n",
    "mylist = [word[0] for word in token if word not in string.punctuation]\n",
    "print(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
